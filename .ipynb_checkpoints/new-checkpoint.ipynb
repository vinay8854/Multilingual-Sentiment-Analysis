{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1e9ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(8+9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8ca030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(8+9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58ae7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\vvv\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started...\n",
      " GPU is available! We will use it for training.\n",
      "Found 1 GPUs.\n",
      "Loading 100,000 random rows from clean_train.csv...\n",
      "Loading 20,000 random rows from clean_test.csv...\n",
      "Loaded 100000 training rows.\n",
      "Loaded 20000 test rows.\n",
      "\n",
      "Mapping labels to integers...\n",
      "Label mapping complete.\n",
      "\n",
      "Converting pandas data to Hugging Face Dataset...\n",
      "Conversion complete. Final DataFrames deleted.\n",
      "\n",
      "Loading tokenizer for xlm-roberta-base...\n",
      "Tokenizing the datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:21<00:00, 4593.82 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:05<00:00, 3538.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "\n",
      "Loading model xlm-roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading complete.\n",
      "\n",
      "Setting up training arguments...\n",
      "\n",
      "--- STARTING MODEL TRAINING (3 Epochs) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 2:06:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>1.089427</td>\n",
       "      <td>0.360950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.118532</td>\n",
       "      <td>0.376050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.063000</td>\n",
       "      <td>1.109724</td>\n",
       "      <td>0.344450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING COMPLETE ---\n",
      "\n",
      "--- EVALUATING MODEL ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- (TRUE) XLM-R MODEL RESULTS ---\n",
      "Overall Accuracy: 34.45%\n",
      "{'eval_loss': 1.1097241640090942, 'eval_accuracy': 0.34445, 'eval_runtime': 66.9834, 'eval_samples_per_second': 298.581, 'eval_steps_per_second': 37.323, 'epoch': 3.0}\n",
      "\n",
      "Total script time: 7746.66 seconds.\n",
      "--- END OF SCRIPT ---\n",
      "\n",
      "--- SAVING THE MODEL ---\n",
      "Model saved to 'my_final_model' folder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Script started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# --- 0. CHECK FOR GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\" GPU is available! We will use it for training.\")\n",
    "    print(f\"Found {torch.cuda.device_count()} GPUs.\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\" FATAL: GPU is NOT available. Exiting.\")\n",
    "    exit() # Exit if no GPU\n",
    "\n",
    "# --- 1. File Paths ---\n",
    "# These are local paths, assuming the files are in the same folder\n",
    "PATH_TRAIN = 'D:/vvv/clean_train.csv'\n",
    "PATH_TEST = 'D:/vvv/clean_test.csv'\n",
    "# ------------------------------------------------\n",
    "\n",
    "# --- 2. Load Data (THE CORRECT WAY) ---\n",
    "try:\n",
    "    print(\"Loading 100,000 random rows from clean_train.csv...\")\n",
    "    df_train_full = pd.read_csv(PATH_TRAIN)\n",
    "    train_df = df_train_full.sample(n=100000, random_state=42)\n",
    "    del df_train_full\n",
    "    \n",
    "    print(\"Loading 20,000 random rows from clean_test.csv...\")\n",
    "    df_test_full = pd.read_csv(PATH_TEST)\n",
    "    test_df = df_test_full.sample(n=20000, random_state=42)\n",
    "    del df_test_full\n",
    "    \n",
    "    print(f\"Loaded {len(train_df)} training rows.\")\n",
    "    print(f\"Loaded {len(test_df)} test rows.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\" Error: File not found. Make sure {PATH_TRAIN} and {PATH_TEST} are in the same folder.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise e\n",
    "\n",
    "# --- 3. Fix the Labels ---\n",
    "print(\"\\nMapping labels to integers...\")\n",
    "label_map = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "}\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "test_df['label'] = test_df['label'].map(label_map)\n",
    "train_df = train_df.dropna(subset=['text', 'label'])\n",
    "test_df = test_df.dropna(subset=['text', 'label'])\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)\n",
    "print(\"Label mapping complete.\")\n",
    "\n",
    "# --- 4. Convert to Hugging Face 'Dataset' format ---\n",
    "print(\"\\nConverting pandas data to Hugging Face Dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "print(\"Conversion complete. Final DataFrames deleted.\")\n",
    "\n",
    "# --- 5. Load Tokenizer & Tokenize ---\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "print(f\"\\nLoading tokenizer for {model_checkpoint}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "print(\"Tokenizing the datasets...\")\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- 6. Load the Model ---\n",
    "print(f\"\\nLoading model {model_checkpoint}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label_map\n",
    ")\n",
    "print(\"Model loading complete.\")\n",
    "\n",
    "# --- 7. Set Up Training ---\n",
    "print(\"\\nSetting up training arguments...\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# We are training for 3 epochs (3 passes over the data)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"xlm-r-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=3,  # Train for 3 epochs\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 8. Train! ---\n",
    "print(\"\\n--- STARTING MODEL TRAINING (3 Epochs) ---\")\n",
    "trainer.train()\n",
    "print(\"--- TRAINING COMPLETE ---\")\n",
    "\n",
    "# --- 9. Evaluate! ---\n",
    "print(\"\\n--- EVALUATING MODEL ---\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n--- (TRUE) XLM-R MODEL RESULTS ---\")\n",
    "print(f\"Overall Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")\n",
    "print(eval_results)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal script time: {total_time:.2f} seconds.\")\n",
    "print(\"--- END OF SCRIPT ---\")\n",
    "\n",
    "print(\"\\n--- SAVING THE MODEL ---\")\n",
    "# This saves the trained model and tokenizer to a folder named 'my_final_model'\n",
    "trainer.save_model(\"my_final_model\")\n",
    "tokenizer.save_pretrained(\"my_final_model\")\n",
    "print(\"Model saved to 'my_final_model' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba5b8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started...\n",
      "âœ… GPU is available! We will use it for training.\n",
      "Found 1 GPUs.\n",
      "Loading full training data to balance it...\n",
      "Found: 1165038 Pos, 1149231 Neg, 833209 Neu\n",
      "\n",
      "--- BALANCED TRAINING DATA ---\n",
      "label\n",
      "positive    33333\n",
      "neutral     33333\n",
      "negative    33333\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "Loading test data...\n",
      "\n",
      "Processing labels...\n",
      "Converting to Hugging Face Dataset...\n",
      "\n",
      "Loading tokenizer (xlm-roberta-base)...\n",
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99999/99999 [00:10<00:00, 9484.68 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:02<00:00, 9477.22 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model (xlm-roberta-base)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up training...\n",
      "\n",
      "--- STARTING BALANCED TRAINING ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:20:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.602264</td>\n",
       "      <td>0.744750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.520300</td>\n",
       "      <td>0.580835</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.589362</td>\n",
       "      <td>0.768200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING COMPLETE ---\n",
      "\n",
      "--- EVALUATING ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 75.60%\n",
      "\n",
      "--- SAVING FINAL MODEL ---\n",
      "Model saved to 'my_final_model'\n",
      "\n",
      "Total script time: 4947.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Script started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- 0. CHECK FOR GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… GPU is available! We will use it for training.\")\n",
    "    print(f\"Found {torch.cuda.device_count()} GPUs.\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"ðŸ›‘ FATAL: GPU is NOT available. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. File Paths ---\n",
    "PATH_TRAIN = 'clean_train.csv'\n",
    "PATH_TEST = 'clean_test.csv'\n",
    "\n",
    "# --- 2. Load Data (YOUR BALANCED METHOD) ---\n",
    "try:\n",
    "    print(\"Loading full training data to balance it...\")\n",
    "    # 1. Load the full training file\n",
    "    df_full = pd.read_csv(PATH_TRAIN)\n",
    "    \n",
    "    # 2. Separate into 3 groups\n",
    "    df_pos = df_full[df_full['label'] == 'positive']\n",
    "    df_neg = df_full[df_full['label'] == 'negative']\n",
    "    df_neu = df_full[df_full['label'] == 'neutral']\n",
    "    \n",
    "    print(f\"Found: {len(df_pos)} Pos, {len(df_neg)} Neg, {len(df_neu)} Neu\")\n",
    "    \n",
    "    # 3. Take 33,333 from EACH (Total ~100k)\n",
    "    # We use min() just in case one class has fewer than 33k rows\n",
    "    n_samples = 33333\n",
    "    df_pos_sample = df_pos.sample(n=n_samples, random_state=42)\n",
    "    df_neg_sample = df_neg.sample(n=n_samples, random_state=42)\n",
    "    df_neu_sample = df_neu.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    # 4. Combine and SHUFFLE (Critical!)\n",
    "    train_df = pd.concat([df_pos_sample, df_neg_sample, df_neu_sample])\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Clear memory\n",
    "    del df_full, df_pos, df_neg, df_neu, df_pos_sample, df_neg_sample, df_neu_sample\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n--- BALANCED TRAINING DATA ---\")\n",
    "    print(train_df['label'].value_counts())\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    # Load Test Data (We keep this random to represent real world)\n",
    "    print(\"Loading test data...\")\n",
    "    df_test_full = pd.read_csv(PATH_TEST)\n",
    "    test_df = df_test_full.sample(n=20000, random_state=42)\n",
    "    del df_test_full\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ðŸ›‘ Error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# --- 3. Fix Labels ---\n",
    "print(\"\\nProcessing labels...\")\n",
    "# Clean labels\n",
    "train_df['label'] = train_df['label'].astype(str).str.strip()\n",
    "test_df['label'] = test_df['label'].astype(str).str.strip()\n",
    "\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "train_df['label'] = train_df['label'].map(label_map)\n",
    "test_df['label'] = test_df['label'].map(label_map)\n",
    "\n",
    "train_df = train_df.dropna(subset=['text', 'label'])\n",
    "test_df = test_df.dropna(subset=['text', 'label'])\n",
    "\n",
    "train_df['text'] = train_df['text'].astype(str)\n",
    "test_df['text'] = test_df['text'].astype(str)\n",
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "# --- 4. Convert to Dataset ---\n",
    "print(\"Converting to Hugging Face Dataset...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "raw_datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "# --- 5. Tokenize ---\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "print(f\"\\nLoading tokenizer ({model_checkpoint})...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# --- 6. Model ---\n",
    "print(f\"\\nLoading model ({model_checkpoint})...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label_map\n",
    ")\n",
    "\n",
    "# --- 7. Training Setup ---\n",
    "print(\"\\nSetting up training...\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"xlm-r-results-balanced\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4, # Smooths out the learning\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- 8. Train ---\n",
    "print(\"\\n--- STARTING BALANCED TRAINING ---\")\n",
    "trainer.train()\n",
    "print(\"--- TRAINING COMPLETE ---\")\n",
    "\n",
    "# --- 9. Evaluate & Save ---\n",
    "print(\"\\n--- EVALUATING ---\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Overall Accuracy: {eval_results['eval_accuracy'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- SAVING FINAL MODEL ---\")\n",
    "trainer.save_model(\"my_final_model\")\n",
    "tokenizer.save_pretrained(\"my_final_model\")\n",
    "print(\"Model saved to 'my_final_model'\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal script time: {total_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84cf262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
